from flask import Flask, request, render_template, jsonify
import os
import pefile
import pandas as pd
import numpy as np
import joblib
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.preprocessing import StandardScaler
import pickle  # To save and load the columns

# Initialize Flask app
app = Flask(__name__)

# Load the pre-trained neural network model
model = load_model('malware_model.h5')  # Replace with your model path
scaler = joblib.load('scaler.pkl')      # Load the scaler you used for training

# Load the saved training columns (This step assumes you saved the columns during training)
with open('training_columns.pkl', 'rb') as f:  # Use the correct path where you saved the columns
    training_columns = pickle.load(f)

# Feature extraction function
def extract_features(file_path):
    try:
        pe = pefile.PE(file_path)
    except pefile.PEFormatError:
        return None
    
    features = {}

    # File Header Features
    features['Machine'] = pe.FILE_HEADER.Machine
    features['NumberOfSections'] = pe.FILE_HEADER.NumberOfSections
    features['SizeOfOptionalHeader'] = pe.FILE_HEADER.SizeOfOptionalHeader
    features['Characteristics'] = pe.FILE_HEADER.Characteristics
    features['TimeDateStamp'] = pe.FILE_HEADER.TimeDateStamp

    # Safely access potentially missing fields with getattr
    features['MajorLinkerVersion'] = getattr(pe.FILE_HEADER, 'MajorLinkerVersion', 0)
    features['MinorLinkerVersion'] = getattr(pe.FILE_HEADER, 'MinorLinkerVersion', 0)

    # Optional Header Features
    features['AddressOfEntryPoint'] = pe.OPTIONAL_HEADER.AddressOfEntryPoint
    features['BaseOfCode'] = pe.OPTIONAL_HEADER.BaseOfCode
    features['BaseOfData'] = getattr(pe.OPTIONAL_HEADER, 'BaseOfData', 0)
    features['ImageBase'] = pe.OPTIONAL_HEADER.ImageBase
    features['SectionAlignment'] = pe.OPTIONAL_HEADER.SectionAlignment
    features['FileAlignment'] = pe.OPTIONAL_HEADER.FileAlignment
    features['SizeOfCode'] = pe.OPTIONAL_HEADER.SizeOfCode
    features['SizeOfInitializedData'] = pe.OPTIONAL_HEADER.SizeOfInitializedData
    features['SizeOfUninitializedData'] = pe.OPTIONAL_HEADER.SizeOfUninitializedData

    # Additional Optional Header Features (add these if required by your model)
    features['MajorOperatingSystemVersion'] = getattr(pe.OPTIONAL_HEADER, 'MajorOperatingSystemVersion', 0)
    features['MinorOperatingSystemVersion'] = getattr(pe.OPTIONAL_HEADER, 'MinorOperatingSystemVersion', 0)
    features['MajorImageVersion'] = getattr(pe.OPTIONAL_HEADER, 'MajorImageVersion', 0)
    features['MinorImageVersion'] = getattr(pe.OPTIONAL_HEADER, 'MinorImageVersion', 0)
    features['MajorSubsystemVersion'] = getattr(pe.OPTIONAL_HEADER, 'MajorSubsystemVersion', 0)
    features['MinorSubsystemVersion'] = getattr(pe.OPTIONAL_HEADER, 'MinorSubsystemVersion', 0)
    features['SizeOfImage'] = getattr(pe.OPTIONAL_HEADER, 'SizeOfImage', 0)
    features['SizeOfHeaders'] = getattr(pe.OPTIONAL_HEADER, 'SizeOfHeaders', 0)
    features['CheckSum'] = getattr(pe.OPTIONAL_HEADER, 'CheckSum', 0)
    features['Subsystem'] = getattr(pe.OPTIONAL_HEADER, 'Subsystem', 0)
    features['DllCharacteristics'] = getattr(pe.OPTIONAL_HEADER, 'DllCharacteristics', 0)
    features['SizeOfStackReserve'] = getattr(pe.OPTIONAL_HEADER, 'SizeOfStackReserve', 0)
    features['SizeOfStackCommit'] = getattr(pe.OPTIONAL_HEADER, 'SizeOfStackCommit', 0)
    features['SizeOfHeapReserve'] = getattr(pe.OPTIONAL_HEADER, 'SizeOfHeapReserve', 0)
    features['SizeOfHeapCommit'] = getattr(pe.OPTIONAL_HEADER, 'SizeOfHeapCommit', 0)
    features['LoaderFlags'] = getattr(pe.OPTIONAL_HEADER, 'LoaderFlags', 0)
    features['NumberOfRvaAndSizes'] = getattr(pe.OPTIONAL_HEADER, 'NumberOfRvaAndSizes', 0)

    # Section Features (Extracting details from each section)
    section_names = []
    section_sizes = []
    section_characteristics = []
    
    for section in pe.sections:
        section_names.append(section.Name.decode().strip())  # Remove trailing null characters
        section_sizes.append(section.SizeOfRawData)
        section_characteristics.append(section.Characteristics)
    
    # Add some stats about sections
    features['NumberOfSections'] = len(pe.sections)  # Reaffirm the number of sections
    features['TotalSectionSize'] = sum(section_sizes)
    features['SectionCharacteristics'] = sum(section_characteristics)  # Example: sum of characteristics, can be customized

    # Add additional features for sections if required by your training columns
    features['SectionsNb'] = len(pe.sections)
    features['SectionsMeanEntropy'] = np.mean([section.get_entropy() for section in pe.sections])
    features['SectionsMinEntropy'] = np.min([section.get_entropy() for section in pe.sections])
    features['SectionsMaxEntropy'] = np.max([section.get_entropy() for section in pe.sections])
    features['SectionsMeanRawsize'] = np.mean(section_sizes)
    features['SectionsMinRawsize'] = np.min(section_sizes)
    features['SectionsMaxRawsize'] = np.max(section_sizes)
    
    # Initialize missing features with 0 or other defaults
    missing_features = [
        'SectionMaxRawsize', 'SectionsMeanVirtualsize', 'SectionsMinVirtualsize', 'SectionMaxVirtualsize',
        'ImportsNbDLL', 'ImportsNb', 'ImportsNbOrdinal', 'ExportNb', 'ResourcesNb', 'ResourcesMeanEntropy',
        'ResourcesMinEntropy', 'ResourcesMaxEntropy', 'ResourcesMeanSize', 'ResourcesMinSize', 'ResourcesMaxSize',
        'LoadConfigurationSize', 'VersionInformationSize'
    ]

    for feature in missing_features:
        features[feature] = 0

    return pd.DataFrame([features])


# Prediction function
def predict_malware(file_path):
    features_df = extract_features(file_path)
    
    if features_df is None:
        return None
    
    # Align columns to match the features used during training
    features_df = features_df[training_columns]  # Keep only the relevant columns used during training
    
    # Ensure that missing columns are added with default values (0) to match training data
    missing_cols = set(training_columns) - set(features_df.columns)
    for col in missing_cols:
        features_df[col] = 0  # Add missing columns with default values (assuming 0)
    
    # Scale the features using the trained scaler
    features_scaled = scaler.transform(features_df)
    
    # Predict using the neural network model
    prediction = model.predict(features_scaled)
    
    # Return 1 for Malicious, 0 for Legitimate
    return int(prediction[0] > 0.5)

# Route for the home page
@app.route('/')
def index():
    return render_template('index.html')

# Route for file upload and prediction
@app.route('/predict', methods=['POST'])
def predict():
    if 'file' not in request.files:
        return jsonify({"error": "No file part"})
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No selected file"})
    
    if file and file.filename.endswith('.exe'):
        file_path = os.path.join('uploads', file.filename)
        file.save(file_path)
        
        result = predict_malware(file_path)
        
        if result is None:
            return jsonify({"error": "Invalid PE file format"})
        
        prediction_result = 'Malicious' if result == 1 else 'Legitimate'
        return jsonify({"prediction": prediction_result})
    
    return jsonify({"error": "Invalid file type. Please upload a .exe file."})

# Start the Flask application
if __name__ == '__main__':
    if not os.path.exists('uploads'):
        os.makedirs('uploads')
    
    # Add the max content length to handle large file uploads (100MB limit)
    app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024  # 100MB
    
    app.run(debug=True)
